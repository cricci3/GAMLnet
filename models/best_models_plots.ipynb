{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "(60215, 21) (60215, 23)\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 100})",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Read in the CSV files\n",
    "\n",
    "# 55% SAR count\n",
    "#file_directory = \"../datasets/60K_60_sar_count/\"\n",
    "\n",
    "# 10% SAR count\n",
    "#file_directory = \"../datasets/60K_10_sar_count/\"\n",
    "\n",
    "# 5% SAR count\n",
    "#file_directory = \"../datasets/60K_05_sar_count/\"\n",
    "\n",
    "# 2% SAR count\n",
    "file_directory = \"../datasets/60K_01_sar_count/\"\n",
    "\n",
    "accounts_df = pd.read_csv(file_directory+\"account_attributes.csv\")\n",
    "transactions_df = pd.read_csv(file_directory+\"transactions.csv\")\n",
    "\n",
    "\n",
    "nodes_df = accounts_df\n",
    "edges_df = transactions_df\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "x_np = nodes_df.to_numpy()\n",
    "x = x_np[:,0:-2]\n",
    "print(x.shape, x_np.shape)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "# Define your graph\n",
    "x = torch.nn.functional.normalize(torch.tensor(x),dim=0).to(torch.float32)  # (n x features)\n",
    "edge_index =  torch.stack([torch.tensor(edges_df.orig_acct.to_numpy()),torch.tensor(edges_df.bene_acct.to_numpy())],dim=-1).T  # Define your edge index\n",
    "#edge_attr = torch.nn.functional.normalize(torch.tensor(np.array(edges_df[['amount','oldBalanceOrig', 'newBalanceOrig', 'oldBalanceDest', 'newBalanceDest','isFlaggedFraud','isUnauthorizedOverdraft','action_CASH_IN','action_CASH_OUT','action_DEBIT','action_PAYMENT','action_TRANSFER']].values,dtype='float32')),dim=0) # edge features\n",
    "edge_weight = torch.nn.functional.normalize(torch.tensor(edges_df.base_amt.to_numpy()),dim=0).long()\n",
    "y =  torch.tensor(nodes_df.node_isSar.to_numpy().astype(int),dtype=torch.long) # target values\n",
    "\n",
    "# Bitflip the tensor\n",
    "#y = y ^ torch.tensor([1], dtype=torch.uint8).expand_as(y)\n",
    "\n",
    "\n",
    "train_size = int(0.35 * len(y))  # 60% of the dataset for training\n",
    "val_size = int(0.15 * len(y))    # 20% of the dataset for validation\n",
    "test_size = len(y) - train_size - val_size  # Remaining 20% for testing\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(y, [train_size, val_size, test_size])\n",
    "\n",
    "# Create masks for train, validation, and test sets\n",
    "train_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "val_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "test_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "\n",
    "train_mask[train_dataset.indices] = True\n",
    "val_mask[val_dataset.indices] = True\n",
    "test_mask[test_dataset.indices] = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "# Load your data into PyTorch Geometric's Data class\n",
    "#data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y,train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "#data = Data(x=x, edge_index=edge_index, y=y,train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "data = Data(x=x, edge_index=edge_index, edge_weight=edge_weight, y=y,train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "data.to(device)\n",
    "\n",
    "from models.MPNN import GNN_MPNN_Model\n",
    "from models.SAGE import GNN_SAGE_Model\n",
    "from models.CONV import GNN_CONV_Model\n",
    "from torch_geometric.nn import Sequential, GAT, GIN, GraphSAGE\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support\n",
    "import json\n",
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 100})'''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MPNN = torch.load(file_directory + 'trained_models/MPNN_best_model.pth')\n",
    "model_SAGE = torch.load(file_directory + 'trained_models/SAGE_best_model.pth')\n",
    "model_GCN = torch.load(file_directory + 'trained_models/GCN_best_model.pth')\n",
    "model_GAT = torch.load(file_directory + 'trained_models/GAT_best_model.pth')\n",
    "model_GIN = torch.load(file_directory + 'trained_models/GIN_best_model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since test and training sets are different now to when we originally trained the models, we need to retrain using the best parameters before doing statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best models json\n",
    "\n",
    "f = open(file_directory + 'trained_models/MPNN_best_model.json')\n",
    "MPNN_JSON = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/SAGE_best_model.json')\n",
    "SAGE_JSON = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GCN_best_model.json')\n",
    "GCN_JSON = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GAT_best_model.json')\n",
    "GAT_JSON = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GIN_best_model.json')\n",
    "GIN_JSON = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ModelGS import ModelGS\n",
    "dataset_num_features = x.size()[1]\n",
    "dataset_num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training MPNN\n",
      "training SGE\n",
      "training GCN\n",
      "training GAT\n",
      "training GIN\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#MPNN\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.15,1.0]).to(device))\n",
    "model = GNN_MPNN_Model(hidden_size=MPNN_JSON['layer size'], input_size=dataset_num_features, output_size=dataset_num_classes, num_layers=MPNN_JSON['num layers']).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=MPNN_JSON['learning rate'], weight_decay=5e-4)\n",
    "epochs = 1000\n",
    "model_MPNN2 = ModelGS(model, optimizer, criterion, data)\n",
    "print(\"training MPNN\")\n",
    "for epoch in range(0, epochs):\n",
    "        train_loss = model_MPNN2.train()\n",
    "        val_loss, val_out, val_pred = model_MPNN2.test(data.val_mask)\n",
    "\n",
    "#SAGE\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.15,1.0]).to(device))\n",
    "model = GNN_SAGE_Model(hidden_size=MPNN_JSON['layer size'], input_size=dataset_num_features, output_size=dataset_num_classes, num_layers=MPNN_JSON['num layers']).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=MPNN_JSON['learning rate'], weight_decay=5e-4)\n",
    "epochs = 1000\n",
    "model_SAGE2 = ModelGS(model, optimizer, criterion, data)\n",
    "print(\"training SAGE\")\n",
    "for epoch in range(0, epochs):\n",
    "        train_loss = model_SAGE2.train()\n",
    "        val_loss, val_out, val_pred = model_SAGE2.test(data.val_mask)\n",
    "\n",
    "\n",
    "#GCN\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.15,1.0]).to(device))\n",
    "model = GNN_CONV_Model(hidden_size=MPNN_JSON['layer size'], input_size=dataset_num_features, output_size=dataset_num_classes, num_layers=MPNN_JSON['num layers']).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=MPNN_JSON['learning rate'], weight_decay=5e-4)\n",
    "epochs = 1000\n",
    "model_CONV2 = ModelGS(model, optimizer, criterion, data)\n",
    "print(\"training GCN\")\n",
    "for epoch in range(0, epochs):\n",
    "        train_loss = model_CONV2.train()\n",
    "        val_loss, val_out, val_pred = model_CONV2.test(data.val_mask)\n",
    "\n",
    "\n",
    "#GAT\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = GAT(in_channels=dataset_num_features, hidden_channels=MPNN_JSON['layer size'], num_layers=MPNN_JSON['num layers'], out_channels=dataset_num_features).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=MPNN_JSON['learning rate'], weight_decay=5e-4)\n",
    "epochs = 1000\n",
    "model_GAT2 = ModelGS(model, optimizer, criterion, data)\n",
    "print(\"training GAT\")\n",
    "for epoch in range(0, epochs):\n",
    "        train_loss = model_GAT2.train()\n",
    "        val_loss, val_out, val_pred = model_GAT2.test(data.val_mask)\n",
    "\n",
    "\n",
    "#GIN\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = GIN(in_channels=dataset_num_features, hidden_channels=MPNN_JSON['layer size'], num_layers=MPNN_JSON['num layers'], out_channels=dataset_num_features).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=MPNN_JSON['learning rate'], weight_decay=5e-4)\n",
    "epochs = 2000\n",
    "model_GIN2 = ModelGS(model, optimizer, criterion, data)\n",
    "print(\"training GIN\")\n",
    "for epoch in range(0, epochs):\n",
    "        train_loss = model_GIN2.train()\n",
    "        val_loss, val_out, val_pred = model_GIN2.test(data.val_mask)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_df = pd.read_csv(file_directory+\"sar_accounts.csv\")\n",
    "ids_cycle = sar_df[\"ACCOUNT_ID\"][sar_df[\"ALERT_TYPE\"]=='cycle'].to_numpy() #super important to convert to numpy array!!!!! keeping as pandas series messes up all indexing!!!\n",
    "ids_fan_in = sar_df[\"ACCOUNT_ID\"][sar_df[\"ALERT_TYPE\"]=='fan_in'].to_numpy()\n",
    "ids_fan_out = sar_df[\"ACCOUNT_ID\"][sar_df[\"ALERT_TYPE\"]=='fan_out'].to_numpy()\n",
    "ids_gather_scatter = sar_df[\"ACCOUNT_ID\"][sar_df[\"ALERT_TYPE\"]=='gather_scatter'].to_numpy()\n",
    "ids_scatter_gather = sar_df[\"ACCOUNT_ID\"][sar_df[\"ALERT_TYPE\"]=='scatter_gather'].to_numpy()\n",
    "set(sar_df[\"ALERT_TYPE\"])\n",
    "\n",
    "ids_test_set = accounts_df['id'][data.test_mask.cpu().numpy()].astype(int).to_numpy()\n",
    "labels_test_set = data.y[data.test_mask].cpu().numpy()\n",
    "\n",
    "_, _, test_pred  = model_MPNN2.test(data.test_mask)\n",
    "predictions_test_set_MPNN = test_pred.cpu().numpy()\n",
    "\n",
    "_, _, test_pred  = model_SAGE2.test(data.test_mask)\n",
    "predictions_test_set_SAGE = test_pred.cpu().numpy()\n",
    "\n",
    "_, _, test_pred  = model_CONV2.test(data.test_mask)\n",
    "predictions_test_set_GCN = test_pred.cpu().numpy()\n",
    "\n",
    "_, _, test_pred  = model_GAT2.test(data.test_mask)\n",
    "predictions_test_set_GAT = test_pred.cpu().numpy()\n",
    "\n",
    "_, _, test_pred  = model_GIN2.test(data.test_mask)\n",
    "predictions_test_set_GIN = test_pred.cpu().numpy()\n",
    "\n",
    "correct_pos_pred_MPNN = np.logical_and(labels_test_set, predictions_test_set_MPNN) # is can only equal 1 if both label and precition were 1\n",
    "correct_pos_pred_SAGE = np.logical_and(labels_test_set, predictions_test_set_SAGE)\n",
    "correct_pos_pred_GCN = np.logical_and(labels_test_set, predictions_test_set_GCN)\n",
    "correct_pos_pred_GAT = np.logical_and(labels_test_set, predictions_test_set_GAT)\n",
    "correct_pos_pred_GIN = np.logical_and(labels_test_set, predictions_test_set_GIN)\n",
    "\n",
    "\n",
    "pred_cycle_MPNN = correct_pos_pred_MPNN[[e in ids_cycle for e in ids_test_set]]\n",
    "pred_fan_in_MPNN = correct_pos_pred_MPNN[[e in ids_fan_in for e in ids_test_set]]\n",
    "pred_fan_out_MPNN = correct_pos_pred_MPNN[[e in ids_fan_out for e in ids_test_set]]\n",
    "pred_gather_scatter_MPNN = correct_pos_pred_MPNN[[e in ids_gather_scatter for e in ids_test_set]]\n",
    "pred_scatter_gather_MPNN = correct_pos_pred_MPNN[[e in ids_scatter_gather for e in ids_test_set]]\n",
    "\n",
    "\n",
    "pred_cycle_SAGE = correct_pos_pred_SAGE[[e in ids_cycle for e in ids_test_set]]\n",
    "pred_fan_in_SAGE = correct_pos_pred_SAGE[[e in ids_fan_in for e in ids_test_set]]\n",
    "pred_fan_out_SAGE = correct_pos_pred_SAGE[[e in ids_fan_out for e in ids_test_set]]\n",
    "pred_gather_scatter_SAGE = correct_pos_pred_SAGE[[e in ids_gather_scatter for e in ids_test_set]]\n",
    "pred_scatter_gather_SAGE = correct_pos_pred_SAGE[[e in ids_scatter_gather for e in ids_test_set]]\n",
    "\n",
    "\n",
    "pred_cycle_GCN = correct_pos_pred_GCN[[e in ids_cycle for e in ids_test_set]]\n",
    "pred_fan_in_GCN = correct_pos_pred_GCN[[e in ids_fan_in for e in ids_test_set]]\n",
    "pred_fan_out_GCN = correct_pos_pred_GCN[[e in ids_fan_out for e in ids_test_set]]\n",
    "pred_gather_scatter_GCN = correct_pos_pred_GCN[[e in ids_gather_scatter for e in ids_test_set]]\n",
    "pred_scatter_gather_GCN = correct_pos_pred_GCN[[e in ids_scatter_gather for e in ids_test_set]]\n",
    "\n",
    "\n",
    "pred_cycle_GAT = correct_pos_pred_GAT[[e in ids_cycle for e in ids_test_set]]\n",
    "pred_fan_in_GAT = correct_pos_pred_GAT[[e in ids_fan_in for e in ids_test_set]]\n",
    "pred_fan_out_GAT = correct_pos_pred_GAT[[e in ids_fan_out for e in ids_test_set]]\n",
    "pred_gather_scatter_GAT = correct_pos_pred_GAT[[e in ids_gather_scatter for e in ids_test_set]]\n",
    "pred_scatter_gather_GAT = correct_pos_pred_GAT[[e in ids_scatter_gather for e in ids_test_set]]\n",
    "\n",
    "\n",
    "pred_cycle_GIN = correct_pos_pred_GIN[[e in ids_cycle for e in ids_test_set]]\n",
    "pred_fan_in_GIN = correct_pos_pred_GIN[[e in ids_fan_in for e in ids_test_set]]\n",
    "pred_fan_out_GIN = correct_pos_pred_GIN[[e in ids_fan_out for e in ids_test_set]]\n",
    "pred_gather_scatter_GIN = correct_pos_pred_GIN[[e in ids_gather_scatter for e in ids_test_set]]\n",
    "pred_scatter_gather_GIN = correct_pos_pred_GIN[[e in ids_scatter_gather for e in ids_test_set]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNN: [0.78 0.73 0.19 0.85 0.88] [0.22 0.27 0.81 0.15 0.12]\n",
      "SAGE: [0.84 0.83 0.37 0.87 0.91] [0.16 0.17 0.63 0.13 0.09]\n",
      "GCN: [0. 0. 0. 0. 0.] [1. 1. 1. 1. 1.]\n",
      "GAT: [0.56 0.62 0.   0.85 0.71] [0.44 0.38 1.   0.15 0.29]\n",
      "GIT: [0. 0. 0. 0. 0.] [1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ntopology_counts = {\\n    'True Positive': tp,\\n    'False Positive': fp,\\n}\\nwidth = 0.6  # the width of the bars: can also be len(x) sequence\\n\\n\\nfig, ax = plt.subplots()\\nbottom = np.zeros(5)\\n\\nfor top, top_count in topology_counts.items():\\n    p = ax.bar(topologies, top_count, width, label=top, bottom=bottom)\\n    bottom += top_count\\n\\n    ax.bar_label(p, label_type='center')\\n\\nax.set_title('Model Performance by Topology Type')\\nax.legend()\\n\\nplt.show()\\n\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topologies = ('Cycle', 'Fan In', 'Fan Out', 'Gather Scatter', 'Scatter Gather')\n",
    "\n",
    "tp_MPNN = np.round(np.array([np.sum(pred_cycle_MPNN)/len(pred_cycle_MPNN), np.sum(pred_fan_in_MPNN)/len(pred_fan_in_MPNN), np.sum(pred_fan_out_MPNN)/len(pred_fan_out_MPNN), np.sum(pred_gather_scatter_MPNN)/len(pred_gather_scatter_MPNN), np.sum(pred_scatter_gather_MPNN)/len(pred_scatter_gather_MPNN)]),2)\n",
    "\n",
    "tp_SAGE = np.round(np.array([np.sum(pred_cycle_SAGE)/len(pred_cycle_SAGE), np.sum(pred_fan_in_SAGE)/len(pred_fan_in_SAGE), np.sum(pred_fan_out_SAGE)/len(pred_fan_out_SAGE), np.sum(pred_gather_scatter_SAGE)/len(pred_gather_scatter_SAGE), np.sum(pred_scatter_gather_SAGE)/len(pred_scatter_gather_SAGE)]),2)\n",
    "\n",
    "tp_GCN = np.round(np.array([np.sum(pred_cycle_GCN)/len(pred_cycle_GCN), np.sum(pred_fan_in_GCN)/len(pred_fan_in_GCN), np.sum(pred_fan_out_GCN)/len(pred_fan_out_GCN), np.sum(pred_gather_scatter_GCN)/len(pred_gather_scatter_GCN), np.sum(pred_scatter_gather_GCN)/len(pred_scatter_gather_GCN)]),2)\n",
    "\n",
    "tp_GAT = np.round(np.array([np.sum(pred_cycle_GAT)/len(pred_cycle_GAT), np.sum(pred_fan_in_GAT)/len(pred_fan_in_GAT), np.sum(pred_fan_out_GAT)/len(pred_fan_out_GAT), np.sum(pred_gather_scatter_GAT)/len(pred_gather_scatter_GAT), np.sum(pred_scatter_gather_GAT)/len(pred_scatter_gather_GAT)]),2)\n",
    "\n",
    "tp_GIN = np.round(np.array([np.sum(pred_cycle_GIN)/len(pred_cycle_GIN), np.sum(pred_fan_in_GIN)/len(pred_fan_in_GIN), np.sum(pred_fan_out_GIN)/len(pred_fan_out_GIN), np.sum(pred_gather_scatter_GIN)/len(pred_gather_scatter_GIN), np.sum(pred_scatter_gather_GIN)/len(pred_scatter_gather_GIN)]),2)\n",
    "\n",
    "\n",
    "fp_MPNN = np.ones(len(tp_MPNN)) - tp_MPNN\n",
    "fp_SAGE = np.ones(len(tp_SAGE)) - tp_SAGE\n",
    "fp_GCN = np.ones(len(tp_GCN)) - tp_GCN\n",
    "fp_GAT = np.ones(len(tp_GAT)) - tp_GAT\n",
    "fp_GIN = np.ones(len(tp_GIN)) - tp_GIN\n",
    "\n",
    "print(\"MPNN:\",tp_MPNN, fp_MPNN)\n",
    "print(\"SAGE:\",tp_SAGE, fp_SAGE)\n",
    "print(\"GCN:\",tp_GCN, fp_GCN)\n",
    "print(\"GAT:\",tp_GAT, fp_GAT)\n",
    "print(\"GIT:\",tp_GIN, fp_GIN)\n",
    "\n",
    "'''\n",
    "topology_counts = {\n",
    "    'True Positive': tp,\n",
    "    'False Positive': fp,\n",
    "}\n",
    "width = 0.6  # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(5)\n",
    "\n",
    "for top, top_count in topology_counts.items():\n",
    "    p = ax.bar(topologies, top_count, width, label=top, bottom=bottom)\n",
    "    bottom += top_count\n",
    "\n",
    "    ax.bar_label(p, label_type='center')\n",
    "\n",
    "ax.set_title('Model Performance by Topology Type')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best models table generation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 55% SAR count\n",
    "file_directory = \"../datasets/60K_60_sar_count/\"\n",
    "\n",
    "f = open(file_directory + 'trained_models/MPNN_best_model.json')\n",
    "MPNN_JSON_D1 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/SAGE_best_model.json')\n",
    "SAGE_JSON_D1 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GCN_best_model.json')\n",
    "GCN_JSON_D1 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GAT_best_model.json')\n",
    "GAT_JSON_D1 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GIN_best_model.json')\n",
    "GIN_JSON_D1 = json.load(f)\n",
    "\n",
    "# 10% SAR count\n",
    "file_directory = \"../datasets/60K_10_sar_count/\"\n",
    "\n",
    "f = open(file_directory + 'trained_models/MPNN_best_model.json')\n",
    "MPNN_JSON_D2 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/SAGE_best_model.json')\n",
    "SAGE_JSON_D2 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GCN_best_model.json')\n",
    "GCN_JSON_D2 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GAT_best_model.json')\n",
    "GAT_JSON_D2 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GIN_best_model.json')\n",
    "GIN_JSON_D2 = json.load(f)\n",
    "\n",
    "# 5% SAR count\n",
    "file_directory = \"../datasets/60K_05_sar_count/\"\n",
    "\n",
    "f = open(file_directory + 'trained_models/MPNN_best_model.json')\n",
    "MPNN_JSON_D3 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/SAGE_best_model.json')\n",
    "SAGE_JSON_D3 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GCN_best_model.json')\n",
    "GCN_JSON_D3 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GAT_best_model.json')\n",
    "GAT_JSON_D3 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GIN_best_model.json')\n",
    "GIN_JSON_D3 = json.load(f)\n",
    "\n",
    "# 2% SAR count\n",
    "file_directory = \"../datasets/60K_01_sar_count/\"\n",
    "\n",
    "f = open(file_directory + 'trained_models/MPNN_best_model.json')\n",
    "MPNN_JSON_D4 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/SAGE_best_model.json')\n",
    "SAGE_JSON_D4 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GCN_best_model.json')\n",
    "GCN_JSON_D4 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GAT_best_model.json')\n",
    "GAT_JSON_D4 = json.load(f)\n",
    "\n",
    "f = open(file_directory + 'trained_models/GIN_best_model.json')\n",
    "GIN_JSON_D4 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset sizes for plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 55% SAR count\n",
    "file_directory = \"../datasets/60K_60_sar_count/\"\n",
    "accounts_df_55 = pd.read_csv(file_directory+\"account_attributes.csv\")\n",
    "transactions_df_55 = pd.read_csv(file_directory+\"transactions.csv\")\n",
    "\n",
    "# 10% SAR count\n",
    "file_directory = \"../datasets/60K_10_sar_count/\"\n",
    "accounts_df_10 = pd.read_csv(file_directory+\"account_attributes.csv\")\n",
    "transactions_df_10 = pd.read_csv(file_directory+\"transactions.csv\")\n",
    "\n",
    "# 5% SAR count\n",
    "#file_directory = \"../datasets/60K_05_sar_count/\"\n",
    "accounts_df_5 = pd.read_csv(file_directory+\"account_attributes.csv\")\n",
    "transactions_df_5 = pd.read_csv(file_directory+\"transactions.csv\")\n",
    "\n",
    "# 2% SAR count\n",
    "#file_directory = \"../datasets/60K_01_sar_count/\"\n",
    "accounts_df_2 = pd.read_csv(file_directory+\"account_attributes.csv\")\n",
    "transactions_df_2 = pd.read_csv(file_directory+\"transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1\n",
      "Number of vertices: 60215\n",
      "Number of anomalous: 32877\n",
      "Number of normal: 27338\n",
      "Number of edges: 1076063\n",
      "\n",
      "Dataset 2\n",
      "Number of vertices: 60215\n",
      "Number of anomalous: 6581\n",
      "Number of normal: 53634\n",
      "Number of edges: 1001080\n",
      "\n",
      "Dataset 3\n",
      "Number of vertices: 60215\n",
      "Number of anomalous: 6581\n",
      "Number of normal: 53634\n",
      "Number of edges: 1001080\n",
      "\n",
      "Dataset 4\n",
      "Number of vertices: 60215\n",
      "Number of anomalous: 6581\n",
      "Number of normal: 53634\n",
      "Number of edges: 1001080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset 1\")\n",
    "print(\"Number of vertices:\",len(accounts_df_55))\n",
    "print(\"Number of anomalous:\",len(accounts_df_55[accounts_df_55[\"node_isSar\"] == 1]))\n",
    "print(\"Number of normal:\",len(accounts_df_55[accounts_df_55[\"node_isSar\"] == 0]))\n",
    "print(\"Number of edges:\", len(transactions_df_55))\n",
    "print()\n",
    "\n",
    "print(\"Dataset 2\")\n",
    "print(\"Number of vertices:\",len(accounts_df_10))\n",
    "print(\"Number of anomalous:\",len(accounts_df_10[accounts_df_10[\"node_isSar\"] == 1]))\n",
    "print(\"Number of normal:\",len(accounts_df_10[accounts_df_10[\"node_isSar\"] == 0]))\n",
    "print(\"Number of edges:\", len(transactions_df_10))\n",
    "print()\n",
    "\n",
    "print(\"Dataset 3\")\n",
    "print(\"Number of vertices:\",len(accounts_df_5))\n",
    "print(\"Number of anomalous:\",len(accounts_df_5[accounts_df_5[\"node_isSar\"] == 1]))\n",
    "print(\"Number of normal:\",len(accounts_df_5[accounts_df_5[\"node_isSar\"] == 0]))\n",
    "print(\"Number of edges:\", len(transactions_df_5))\n",
    "print()\n",
    "\n",
    "print(\"Dataset 4\")\n",
    "print(\"Number of vertices:\",len(accounts_df_2))\n",
    "print(\"Number of anomalous:\",len(accounts_df_2[accounts_df_2[\"node_isSar\"] == 1]))\n",
    "print(\"Number of normal:\",len(accounts_df_2[accounts_df_2[\"node_isSar\"] == 0]))\n",
    "print(\"Number of edges:\", len(transactions_df_2))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
