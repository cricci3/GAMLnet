{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import model_helper as mh\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_gridsearch(ds):\n",
    "    param_grid = {'bootstrap': [True, False],\n",
    "        'max_depth': [10, 30, 50, 70, 90, None],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'n_estimators': [20, 50, 100]}\n",
    "    \n",
    "    trainX = ds.data.x[ds.data.train_mask + ds.data.val_mask].detach().cpu().numpy()\n",
    "    trainY = ds.data.y[ds.data.train_mask + ds.data.val_mask].detach().cpu().numpy()\n",
    "\n",
    "    testX = ds.data.x[ds.data.test_mask].detach().cpu().numpy()\n",
    "    testY = ds.data.y[ds.data.test_mask].detach().cpu().numpy()\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "    grid_search.fit(trainX, trainY)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    #clf = RandomForestClassifier(bootstrap=best_params['bootstrap'],max_depth=best_params['max_depth'],max_features=best_params['max_features'],min_samples_leaf=best_params['min_samples_leaf'],min_samples_split=best_params['min_samples_split'],n_estimators=best_params['n_estimators'])\n",
    "    #st = time.time()\n",
    "    #clf.fit(trainX, trainY)\n",
    "    #et = time.time()\n",
    "    #elapsed_time = et - st\n",
    "    #best_model = clf\n",
    "    \n",
    "    test_pred = best_model.predict(testX)\n",
    "    y_pred_prob = best_model.predict_proba(testX)\n",
    "    y_pred_prob = y_pred_prob[:,1]\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(testY, test_pred, average='binary')                     \n",
    "    fpr, tpr, thresholds = roc_curve(testY, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    results = {\n",
    "        'test_f1':f1_score,\n",
    "        'test_auc':roc_auc,\n",
    "        'test_precision':precision,\n",
    "        'test_recall':recall,\n",
    "        'bootstrap':best_params['bootstrap'],\n",
    "        'max_depth':best_params['max_depth'],\n",
    "        'n_estimators':best_params['n_estimators'],\n",
    "        'max_features':best_params['max_features'],\n",
    "        'min_samples_leaf':best_params['min_samples_leaf'],\n",
    "        'min_samples_split':best_params['min_samples_split']\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xg_gridsearch(ds):\n",
    "    trainX = ds.data.x[ds.data.train_mask + ds.data.val_mask].detach().cpu().numpy()\n",
    "    trainY = ds.data.y[ds.data.train_mask + ds.data.val_mask].detach().cpu().numpy()\n",
    "\n",
    "    testX = ds.data.x[ds.data.test_mask].detach().cpu().numpy()\n",
    "    testY = ds.data.y[ds.data.test_mask].detach().cpu().numpy()\n",
    "\n",
    "    clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "    params = {\n",
    "        \"gamma\": [0,0.2,0.4,0.6],\n",
    "        'max_depth': [10, 30, 50, 70, 90, None],\n",
    "        'n_estimators': [20, 50, 100]}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=clf, param_grid=params, cv=3, n_jobs=-1)\n",
    "    grid_search.fit(trainX, trainY)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    #clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42,gamma=best_params[\"gamma\"], max_depth=best_params[\"max_depth\"],n_estimators=best_params[\"n_estimators\"])\n",
    "    #st = time.time()\n",
    "    #clf.fit(trainX, trainY)\n",
    "    #et = time.time()\n",
    "    #elapsed_time = et - st\n",
    "\n",
    "    test_pred = best_model.predict(testX)\n",
    "    test_pred = np.round(test_pred)\n",
    "    print(np.sum(test_pred))\n",
    "\n",
    "    y_pred_prob = best_model.predict_proba(testX)\n",
    "    y_pred_prob = y_pred_prob[:,1]\n",
    "\n",
    "    #test_acc, test_out, test_pred  = model_gs.test(data.test_mask)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(testY, test_pred, average='binary')                     \n",
    "    fpr, tpr, thresholds = roc_curve(testY, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    results = {\n",
    "        'test_f1':f1_score,\n",
    "        'test_auc':roc_auc,\n",
    "        'test_precision':precision,\n",
    "        'test_recall':recall,\n",
    "        'max_depth':best_params['max_depth'],\n",
    "        'n_estimators':best_params['n_estimators'],\n",
    "        'gamma':best_params['gamma']\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_gridsearch(ds_split_seeds, dataset_folder, method, gnn_params, ds_name, results_path, gs_results):\n",
    "    epochs = 5000\n",
    "    results = {\n",
    "        'test_f1':None,\n",
    "        'test_auc':None,\n",
    "        'test_precision':None,\n",
    "        'test_recall':None,\n",
    "        'lr':None,\n",
    "        'w':None,\n",
    "        'K':None,\n",
    "        'F':None,\n",
    "        'K1':None,\n",
    "        'K2':None,\n",
    "        'F1':None,\n",
    "        'F2':None,\n",
    "        'epoch':None\n",
    "    }\n",
    "    best_average_test_f1 = 0\n",
    "    count = 1\n",
    "    for lr in gnn_params['lr']:\n",
    "        for w in gnn_params['w']:\n",
    "            if method == \"GINSAGE\":\n",
    "                \n",
    "                for K1 in gnn_params['K1']:\n",
    "                    for K2 in gnn_params['K2']:\n",
    "                        for F1 in gnn_params['F1']:\n",
    "                            for F2 in gnn_params['F2']:\n",
    "                                f1_scores_for_group = []\n",
    "                                best_epochs_for_group = []\n",
    "                                auc_scores_for_group = []\n",
    "                                precision_scores_for_group = []\n",
    "                                recall_scores_for_group = []\n",
    "                                for seed in ds_split_seeds:\n",
    "                                    print(method, count, 'out of', len(gnn_params['lr'])*len(gnn_params['w'])*len(gnn_params['K1'])*len(gnn_params['K2'])*len(gnn_params['F1'])*len(gnn_params['F2'])*len(ds_split_seeds))\n",
    "                                    \n",
    "                                    ds = mh.Dataset()\n",
    "                                    ds.load_dataset(folder=dataset_folder+ds_name,splits=[0.5,0.2,0.3],split_type=\"normal\",split_seed=seed)\n",
    "                                    gin_feature_indices = [ds.feature_labels.index(item) for item in ['node_deg_out_unique', 'node_deg_in_unique']]\n",
    "                                    #gin_feature_indices = [ds.feature_labels.index(item) for item in ['node_deg_out', 'node_deg_in']]\n",
    "                                    #gin_feature_indices = [-1]\n",
    "                                    \n",
    "                                    model = mh.Model(ds.data, gridsearch_flag=True)\n",
    "                                    \n",
    "                                    additional_params = {'K1':K1, 'K2':K2, 'F1':F1, 'F2':F2, 'gin_feature_indices':gin_feature_indices}\n",
    "                                    model.w = w\n",
    "                                    model.lr = lr\n",
    "                                    model.load_model(\"GINSAGE\",additional_params=additional_params)\n",
    "                                    model.train_model(epochs=epochs)\n",
    "                                    f1_scores_for_group.append(model.gridsearch_results['test_f1'])\n",
    "                                    best_epochs_for_group.append(model.gridsearch_results['test_epoch'])\n",
    "                                    auc_scores_for_group.append(model.gridsearch_results['test_auc'])\n",
    "                                    precision_scores_for_group.append(model.gridsearch_results['test_precision'])\n",
    "                                    recall_scores_for_group.append(model.gridsearch_results['test_recall'])\n",
    "                                    count+=1\n",
    "\n",
    "                                if np.mean(f1_scores_for_group) > best_average_test_f1:\n",
    "                                    best_average_test_f1 = np.mean(f1_scores_for_group)\n",
    "                                    results = {\n",
    "                                    'mean_f1':np.mean(f1_scores_for_group),\n",
    "                                    'test_f1':f1_scores_for_group,\n",
    "                                    'test_auc':auc_scores_for_group,\n",
    "                                    'test_precision':precision_scores_for_group,\n",
    "                                    'test_recall':recall_scores_for_group,\n",
    "                                    'epoch':best_epochs_for_group,\n",
    "                                    'lr':lr,\n",
    "                                    'w':w,\n",
    "                                    'K':None,\n",
    "                                    'F':None,\n",
    "                                    'K1':K1,\n",
    "                                    'K2':K2,\n",
    "                                    'F1':F1,\n",
    "                                    'F2':F2\n",
    "                                    }\n",
    "                                for ele in results:\n",
    "                                    gs_results[ds_name][method][ele] = results[ele]\n",
    "                                    print(ele,results[ele])\n",
    "                                with open(results_path, 'w') as file:\n",
    "                                    json.dump(gs_results, file, indent=2)\n",
    "                                    \n",
    "                                \n",
    "            else:\n",
    "                for K in gnn_params['K']:\n",
    "                    for F in gnn_params['F']:\n",
    "                        f1_scores_for_group = []\n",
    "                        best_epochs_for_group = []\n",
    "                        auc_scores_for_group = []\n",
    "                        precision_scores_for_group = []\n",
    "                        recall_scores_for_group = []\n",
    "                        for seed in ds_split_seeds:\n",
    "                            print(method, count, 'out of', len(gnn_params['lr'])*len(gnn_params['w'])*len(gnn_params['K'])*len(gnn_params['F'])*len(ds_split_seeds))\n",
    "\n",
    "\n",
    "                            ds = mh.Dataset()\n",
    "                            ds.load_dataset(folder=dataset_folder+ds_name,splits=[0.5,0.2,0.3],split_type=\"normal\",split_seed=seed)\n",
    "\n",
    "                            model = mh.Model(ds.data, gridsearch_flag=True)\n",
    "                            model.w = w\n",
    "                            model.lr = lr\n",
    "                            model.load_model(method,K=K,F=F)\n",
    "                            model.train_model(epochs=epochs)\n",
    "                            f1_scores_for_group.append(model.gridsearch_results['test_f1'])\n",
    "                            best_epochs_for_group.append(model.gridsearch_results['test_epoch'])\n",
    "                            auc_scores_for_group.append(model.gridsearch_results['test_auc'])\n",
    "                            precision_scores_for_group.append(model.gridsearch_results['test_precision'])\n",
    "                            recall_scores_for_group.append(model.gridsearch_results['test_recall'])\n",
    "                            count+=1\n",
    "\n",
    "                        if np.mean(f1_scores_for_group) > best_average_test_f1:\n",
    "                            best_average_test_f1 = np.mean(f1_scores_for_group)\n",
    "                            results = {\n",
    "                            'mean_f1':np.mean(f1_scores_for_group),\n",
    "                            'test_f1':f1_scores_for_group,\n",
    "                            'test_auc':auc_scores_for_group,\n",
    "                            'test_precision':precision_scores_for_group,\n",
    "                            'test_recall':recall_scores_for_group,\n",
    "                            'epoch':best_epochs_for_group,\n",
    "                            'lr':lr,\n",
    "                            'w':w,\n",
    "                            'K':K,\n",
    "                            'F':F,\n",
    "                            'K1':None,\n",
    "                            'K2':None,\n",
    "                            'F1':None,\n",
    "                            'F2':None\n",
    "                            }\n",
    "                        for ele in results:\n",
    "                            gs_results[ds_name][method][ele] = results[ele]\n",
    "                            print(ele,results[ele])\n",
    "                        with open(results_path, 'w') as file:\n",
    "                            json.dump(gs_results, file, indent=2)\n",
    "                            \n",
    "                        \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_params = {\n",
    "    'lr':[0.001,0.005,0.01,0.05],\n",
    "    'w':[0.4],\n",
    "    'K':[2,3,4,5,6],\n",
    "    'F':[8,16,32,64],\n",
    "    'K1':[1,2,3],\n",
    "    'K2':[4,5,6],\n",
    "    'F1':[8,16],\n",
    "    'F2':[16,32,64]\n",
    "}\n",
    "gnn_params = {\n",
    "    'lr':[0.01, 0.005],\n",
    "    'w':[0.44],\n",
    "    'K':[1,2,3],\n",
    "    'F':[16,32,8],\n",
    "    'K1':[2,3],\n",
    "    'K2':[4,5,6],\n",
    "    'F1':[8,16,32],\n",
    "    'F2':[32,64]\n",
    "}\n",
    "#'w':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "\n",
    "gnn_params = {\n",
    "    'lr':[0.005],\n",
    "    'w':[.96],\n",
    "    'K':[3],\n",
    "    'F':[32],\n",
    "    'K1':[3],\n",
    "    'K2':[5],\n",
    "    'F1':[8],\n",
    "    'F2':[64]\n",
    "}\n",
    "\n",
    "gnn_params_copy = gnn_params\n",
    "weights_temp = [.96, .79, .44, .26]\n",
    "weights_temp = [.44, .26, .79, .96]\n",
    "#weights_temp = [.96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_toposearch(ds,param_grid,folder):\n",
    "    '''\n",
    "    param_grid = {'bootstrap': [True],\n",
    "        'max_depth': [70],\n",
    "        'max_features': ['sqrt'],\n",
    "        'min_samples_leaf': [4],\n",
    "        'min_samples_split': [5],\n",
    "        'n_estimators': [50]}\n",
    "    '''\n",
    "    trainX = ds.data.x[ds.data.train_mask + ds.data.val_mask].detach().cpu().numpy()\n",
    "    trainY = ds.data.y[ds.data.train_mask + ds.data.val_mask].detach().cpu().numpy()\n",
    "\n",
    "    testX = ds.data.x[ds.data.test_mask].detach().cpu().numpy()\n",
    "    testY = ds.data.y[ds.data.test_mask].detach().cpu().numpy()\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "    grid_search.fit(trainX, trainY)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    #clf = RandomForestClassifier(bootstrap=best_params['bootstrap'],max_depth=best_params['max_depth'],max_features=best_params['max_features'],min_samples_leaf=best_params['min_samples_leaf'],min_samples_split=best_params['min_samples_split'],n_estimators=best_params['n_estimators'])\n",
    "    #st = time.time()\n",
    "    #clf.fit(trainX, trainY)\n",
    "    #et = time.time()\n",
    "    #elapsed_time = et - st\n",
    "    #best_model = clf\n",
    "    \n",
    "    test_pred = best_model.predict(testX)\n",
    "    y_pred_prob = best_model.predict_proba(testX)\n",
    "    y_pred_prob = y_pred_prob[:,1]\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(testY, test_pred, average='binary')                     \n",
    "    fpr, tpr, thresholds = roc_curve(testY, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    results = {}\n",
    "    results[\"f1\"] = f1_score\n",
    "\n",
    "    return results\n",
    "\n",
    "def xg_toposearch(ds,params,folder):\n",
    "    trainX = ds.data.x[ds.data.train_mask + ds.data.val_mask].detach().cpu().numpy()\n",
    "    trainY = ds.data.y[ds.data.train_mask + ds.data.val_mask].detach().cpu().numpy()\n",
    "\n",
    "    testX = ds.data.x[ds.data.test_mask].detach().cpu().numpy()\n",
    "    testY = ds.data.y[ds.data.test_mask].detach().cpu().numpy()\n",
    "\n",
    "    clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "    '''\n",
    "    params = {\n",
    "        \"gamma\": [0],\n",
    "        'max_depth': [30],\n",
    "        'n_estimators': [100]}\n",
    "    '''\n",
    "    grid_search = GridSearchCV(estimator=clf, param_grid=params, cv=3, n_jobs=-1)\n",
    "    grid_search.fit(trainX, trainY)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    #clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42,gamma=best_params[\"gamma\"], max_depth=best_params[\"max_depth\"],n_estimators=best_params[\"n_estimators\"])\n",
    "    #st = time.time()\n",
    "    #clf.fit(trainX, trainY)\n",
    "    #et = time.time()\n",
    "    #elapsed_time = et - st\n",
    "\n",
    "    test_pred = best_model.predict(testX)\n",
    "    test_pred = np.round(test_pred)\n",
    "    print(np.sum(test_pred))\n",
    "\n",
    "    y_pred_prob = best_model.predict_proba(testX)\n",
    "    y_pred_prob = y_pred_prob[:,1]\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(testY, test_pred, average='binary')                     \n",
    "    fpr, tpr, thresholds = roc_curve(testY, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    results = {}\n",
    "    results[\"f1\"] = f1_score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing JSON gridsearch results file loaded.\n",
      "device: cuda:0\n",
      "loading dataset v2/8K_5_v2 | length: 8032 | fraud percentage (%): 4.38\n",
      "Running Dataset 8K_5_v2\n",
      "    Running method GINSAGE\n",
      "device: cuda:0\n",
      "GINSAGE 1 out of 10\n",
      "device: cuda:0\n",
      "loading dataset v2/8K_5_v2 | length: 8032 | fraud percentage (%): 4.38\n",
      "device: cuda:0\n",
      "{'K1': 2, 'K2': 6, 'F1': 8, 'F2': 64, 'gin_feature_indices': [19, 11]}\n",
      "model training starting...\n"
     ]
    }
   ],
   "source": [
    "#loop format\n",
    "#loop through datasets\n",
    "#loop through models\n",
    "#gridsearch model\n",
    "\n",
    "dataset_folder = 'v2/'\n",
    "#dataset_folder = 'thesis_datasets/'\n",
    "#datasets = ['128K_05_v2_2','128K_1_v2_2','128K_5_v2_2','128K_10_v2_2']\n",
    "#datasets = ['128K_5_v2_2','128K_10_v2_2','128K_1_v2_2','128K_05_v2_2']\n",
    "datasets = ['8K_5_v2','16K_5_v2','32K_5_v2','64K_5_v2','128K_5_v2_2','128K_10_v2_2','128K_1_v2_2','128K_05_v2_2']\n",
    "#datasets = ['128K_5_v2_2']\n",
    "#methods = ['SAGE','XG','RF']\n",
    "#methods = ['GINSAGE','SAGE','XG','RF']\n",
    "#methods = ['SAGE','GINSAGE']\n",
    "methods = ['GINSAGE','GIN','SAGE','XG','RF']\n",
    "#methods = ['RF','XG']\n",
    "\n",
    "now = datetime.now()\n",
    "current_date = now.strftime(\"%Y-%m-%d\")\n",
    "current_time = now.strftime(\"%H-%M-%S\")\n",
    "result_string = f\"{current_date}_{current_time}\"\n",
    "\n",
    "results_path = 'gridsearch_results.json'\n",
    "#results_path = 'gridsearch_results/gridsearch_results_'+result_string+'.json'\n",
    "\n",
    "\n",
    "gs_results = {}\n",
    "\n",
    "try:\n",
    "    # Try to open the existing JSON file\n",
    "    with open(results_path, 'r') as file:\n",
    "        gs_results = json.load(file)\n",
    "        print(\"Existing JSON gridsearch results file loaded.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    gs_results = {}\n",
    "    print(\"JSON gridsearch results file not found. Creating a new one.\")\n",
    "    with open(results_path, 'w') as file:\n",
    "        json.dump(gs_results, file, indent=2)\n",
    "\n",
    "count = -1\n",
    "for ds_name in datasets:\n",
    "    #remove this after\n",
    "    #count+=1\n",
    "    #gnn_params['w'] = [weights_temp[count]]\n",
    "    #~~~\n",
    "\n",
    "    split_seeds = [0,111]\n",
    "    split_seeds = [0,111,222,333,444,555,666,777,888,999]\n",
    "    #split_seeds = [0]\n",
    "    ds = mh.Dataset()\n",
    "    ds.load_dataset(folder=dataset_folder+ds_name,splits=[0.5,0.2,0.3],split_type=\"normal\")\n",
    "    print('Running Dataset',ds_name)\n",
    "    if ds_name not in gs_results:\n",
    "        gs_results[ds_name] = {}\n",
    "\n",
    "    for method in methods:\n",
    "        model_path = './best_models/'+dataset_folder+ds_name+'_'+method+'.pth'\n",
    "        print('    Running method',method)\n",
    "        #grid search\n",
    "        \n",
    "        #gs_results[ds_name][method] = {}\n",
    "        \n",
    "        results = {}\n",
    "        if method in ['GIN','SAGE','MPNN','GCN','GAT','GINSAGE']:\n",
    "            gnn_params = {\n",
    "                'lr':[gs_results[ds_name][method]['lr']],\n",
    "                'w':[gs_results[ds_name][method]['w']],\n",
    "                'K':[gs_results[ds_name][method]['K']],\n",
    "                'F':[gs_results[ds_name][method]['F']],\n",
    "                'K1':[gs_results[ds_name][method]['K1']],\n",
    "                'K2':[gs_results[ds_name][method]['K2']],\n",
    "                'F1':[gs_results[ds_name][method]['F1']],\n",
    "                'F2':[gs_results[ds_name][method]['F2']],\n",
    "                'mean_f1':gs_results[ds_name][method]['mean_f1'],\n",
    "                'epochs':gs_results[ds_name][method]['epoch']\n",
    "            }\n",
    "            model = mh.Model(ds.data, gridsearch_flag=True)\n",
    "            results = gnn_gridsearch(split_seeds, dataset_folder, method, gnn_params, ds_name, results_path, gs_results)\n",
    "            '''\n",
    "            try:\n",
    "                results = gnn_gridsearch(model, ds, method, gnn_params)\n",
    "            except:\n",
    "                print(method,'on dataset',ds_name,'has crashed. Continuing to next seach.')\n",
    "            '''\n",
    "        elif method == 'XG':\n",
    "            xg_params = {\n",
    "                \"gamma\": [gs_results[ds_name][method]['gamma']],\n",
    "                'max_depth': [gs_results[ds_name][method]['max_depth']],\n",
    "                'n_estimators': [gs_results[ds_name][method]['n_estimators']]}\n",
    "            f1s = [] \n",
    "            for seed in split_seeds:\n",
    "                print(\"seed\",seed)\n",
    "                ds = mh.Dataset()\n",
    "                ds.load_dataset(folder=dataset_folder+ds_name,splits=[0.5,0.2,0.3],split_type=\"normal\",split_seed=seed)\n",
    "                results2 = xg_toposearch(ds, xg_params,\"v2/\"+ds_name)\n",
    "                f1s.append(results2[\"f1\"])\n",
    "            results[\"f1_scores\"] = f1s\n",
    "            results[\"mean_f1\"] = np.mean(f1s)\n",
    "        elif method == 'RF':\n",
    "            rf_params = {'bootstrap': [gs_results[ds_name][method]['bootstrap']],\n",
    "                'max_depth': [gs_results[ds_name][method]['max_depth']],\n",
    "                'max_features': [gs_results[ds_name][method]['max_features']],\n",
    "                'min_samples_leaf': [gs_results[ds_name][method]['min_samples_leaf']],\n",
    "                'min_samples_split': [gs_results[ds_name][method]['min_samples_split']],\n",
    "                'n_estimators': [gs_results[ds_name][method]['n_estimators']]}\n",
    "            f1s = []\n",
    "            for seed in split_seeds:\n",
    "                print(\"seed\",seed)\n",
    "                ds = mh.Dataset()\n",
    "                ds.load_dataset(folder=dataset_folder+ds_name,splits=[0.5,0.2,0.3],split_type=\"normal\",split_seed=seed)\n",
    "                results2 = rf_toposearch(ds,rf_params,\"v2/\"+ds_name)\n",
    "                f1s.append(results2[\"f1\"])\n",
    "            results[\"f1_scores\"] = f1s\n",
    "            results[\"mean_f1\"] = np.mean(f1s)\n",
    "        else:\n",
    "            print('method',method,'does not exist!')\n",
    "\n",
    "        \n",
    "        \n",
    "        for ele in results:\n",
    "            gs_results[ds_name][method][ele] = results[ele]\n",
    "            print(ele,results[ele])\n",
    "\n",
    "        with open(results_path, 'w') as file:\n",
    "            json.dump(gs_results, file, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128002, 27])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_f1': 0.8276669931216768,\n",
       " 'test_f1': [0.828786999419617,\n",
       "  0.8412880966072455,\n",
       "  0.8145281333730277,\n",
       "  0.8248242127789668,\n",
       "  0.828294687408277,\n",
       "  0.8342089900758903,\n",
       "  0.8217378321887131],\n",
       " 'test_auc': [0.8678542016626297,\n",
       "  0.8745943554443908,\n",
       "  0.8545909685806059,\n",
       "  0.864160909867488,\n",
       "  0.8751723765077881,\n",
       "  0.8707201902743981,\n",
       "  0.8611052905544044],\n",
       " 'test_precision': [0.9450694904037061,\n",
       "  0.9562091503267974,\n",
       "  0.9533101045296167,\n",
       "  0.9473314606741573,\n",
       "  0.9192182410423453,\n",
       "  0.9501329787234043,\n",
       "  0.9496204278812974],\n",
       " 'test_recall': [0.737984496124031,\n",
       "  0.7510266940451745,\n",
       "  0.7110187110187111,\n",
       "  0.7303735787763942,\n",
       "  0.7537393162393162,\n",
       "  0.7434963579604579,\n",
       "  0.7242105263157895],\n",
       " 'epoch': [4810, 4510, 1870, 2900, 4400, 4960, 4940],\n",
       " 'lr': 0.005,\n",
       " 'w': 0.44,\n",
       " 'K': None,\n",
       " 'F': None,\n",
       " 'K1': 2,\n",
       " 'K2': 5,\n",
       " 'F1': 8,\n",
       " 'F2': 32}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mcheck_topology_performance(folder\u001b[38;5;241m=\u001b[39m\u001b[43mds1\u001b[49m\u001b[38;5;241m.\u001b[39mfolder, data\u001b[38;5;241m=\u001b[39mds1\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds1' is not defined"
     ]
    }
   ],
   "source": [
    "model.check_topology_performance(folder=ds1.folder, data=ds1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_f1': 0.8294841855240406,\n",
       " 'test_f1': [0.8183807439824947, 0.8624708624708625, 0.8076009501187648],\n",
       " 'test_auc': [0.8690524190029358, 0.8820119623429218, 0.8543327460911694],\n",
       " 'test_precision': [0.9121951219512195,\n",
       "  0.9893048128342246,\n",
       "  0.9340659340659341],\n",
       " 'test_recall': [0.7420634920634921, 0.7644628099173554, 0.7112970711297071],\n",
       " 'epoch': [1040, 1220, 4850],\n",
       " 'lr': 0.01,\n",
       " 'w': 0.44,\n",
       " 'K': None,\n",
       " 'F': None,\n",
       " 'K1': 3,\n",
       " 'K2': 5,\n",
       " 'F1': 8,\n",
       " 'F2': 32}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "loading dataset v2/128K_05_v2_2 | length: 128002 | fraud percentage (%): 0.49\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "ds = mh.Dataset()\n",
    "ds.load_dataset(folder=dataset_folder+'128K_05_v2_2',splits=[0.5,0.2,0.3],split_type=\"normal\")\n",
    "    \n",
    "method = 'GINSAGE'\n",
    "model = mh.Model(ds.data, gridsearch_flag=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.0.1+cu118\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: d:\\anaconda\\envs\\anaconda\\envs\\gdl2\\lib\\site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: tensordict, torchaudio, torchrl, torchvision\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "loading dataset v2/16K_5_v2 | length: 15995 | fraud percentage (%): 5.0\n",
      "device: cuda:0\n",
      "loading dataset v2/16K_5_v2 | length: 15995 | fraud percentage (%): 5.0\n",
      "device: cuda:0\n",
      "loading dataset v2/16K_5_v2 | length: 15995 | fraud percentage (%): 5.0\n"
     ]
    }
   ],
   "source": [
    "ds1 = mh.Dataset()\n",
    "ds1.load_dataset(folder='v2/'+'16K_5_v2',splits=[0.5,0.2,0.3],split_type=\"normal\",split_seed=0)\n",
    "\n",
    "ds2 = mh.Dataset()\n",
    "ds2.load_dataset(folder='v2/'+'16K_5_v2',splits=[0.5,0.2,0.3],split_type=\"normal\",split_seed=0)\n",
    "\n",
    "ds3 = mh.Dataset()\n",
    "ds3.load_dataset(folder='v2/'+'16K_5_v2',splits=[0.5,0.2,0.3],split_type=\"normal\",split_seed=294)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(440, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ds1.data.y[ds1.data.test_mask] != ds3.data.y[ds3.data.test_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "loading dataset v2/16K_5_v2 | length: 15995 | fraud percentage (%): 5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds1 = mh.Dataset()\n",
    "ds1.load_dataset(folder='v2/'+'16K_5_v2',splits=[0.5,0.2,0.3],split_type=\"normal\",split_seed=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0525, device='cuda:0') tensor(420, device='cuda:0')\n",
      "tensor(0.0416, device='cuda:0') tensor(133, device='cuda:0')\n",
      "tensor(0.0515, device='cuda:0') tensor(247, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(sum(ds1.data.y[ds1.data.train_mask])/sum(ds1.data.train_mask), sum(ds1.data.y[ds1.data.train_mask]))\n",
    "print(sum(ds1.data.y[ds1.data.val_mask])/sum(ds1.data.val_mask), sum(ds1.data.y[ds1.data.val_mask]))\n",
    "print(sum(ds1.data.y[ds1.data.test_mask])/sum(ds1.data.test_mask), sum(ds1.data.y[ds1.data.test_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
